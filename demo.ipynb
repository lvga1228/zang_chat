{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7de78ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape = torch.Size([1, 8, 50304])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='mps:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "generated tokens: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from zangchat.gpt import GPT, GPTConfig  # 注意用修正后的名字\n",
    "\n",
    "def main():\n",
    "    config = GPTConfig \n",
    "    model = GPT(config)\n",
    "    model.init_weights()\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 假造一批 token\n",
    "    idx = torch.randint(0, config.vocab_size, (1, 8), device=device)  # (B=1, T=8)\n",
    "    logits = model(idx)\n",
    "    print(\"logits.shape =\", logits.shape)  # 期望: (1, 8, vocab_size)\n",
    "    # 一定全是0因为我们最后的MATRIX_WEIGHT:lm_head赋值为0\n",
    "    print(logits)\n",
    "\n",
    "    # 测试 generate\n",
    "    tokens = [1, 2, 3]\n",
    "    out = []\n",
    "    for t in model.generate(tokens, max_tokens=10, temperature=0.0):\n",
    "        out.append(t)\n",
    "    print(\"generated tokens:\", out)\n",
    "    print(device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d18abcb",
   "metadata": {},
   "source": [
    "# Demo of GPT done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9116b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 995, 0]\n",
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "from zangchat.tokenizer import RustBPETokenizer\n",
    "\n",
    "tok = RustBPETokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "ids = tok.encode(\"Hello world!\")\n",
    "print(ids)\n",
    "\n",
    "print(tok.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0c204",
   "metadata": {},
   "source": [
    "# Demo of tokenizer done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanochat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
